{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 캐글 자전거 수요 예측 : https://www.kaggle.com/competitions/bike-sharing-demand/overview\n",
    "### 목표 : 전처리 방법 변경 및 모델을 Tensorflow 딥러닝 모델로 변경하여 제출 후 스코어 0.8 이하 도달하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Train Loss: 4.0886, Val Loss: 3.4837\n",
      "Epoch 2/1000, Train Loss: 2.9277, Val Loss: 2.4314\n",
      "Epoch 3/1000, Train Loss: 2.1199, Val Loss: 1.8543\n",
      "Epoch 4/1000, Train Loss: 1.6721, Val Loss: 1.5286\n",
      "Epoch 5/1000, Train Loss: 1.4084, Val Loss: 1.3366\n",
      "Epoch 6/1000, Train Loss: 1.2542, Val Loss: 1.2212\n",
      "Epoch 7/1000, Train Loss: 1.1596, Val Loss: 1.1489\n",
      "Epoch 8/1000, Train Loss: 1.1022, Val Loss: 1.1006\n",
      "Epoch 9/1000, Train Loss: 1.0665, Val Loss: 1.0678\n",
      "Epoch 10/1000, Train Loss: 1.0302, Val Loss: 1.0445\n",
      "Epoch 11/1000, Train Loss: 1.0097, Val Loss: 1.0274\n",
      "Epoch 12/1000, Train Loss: 0.9977, Val Loss: 1.0143\n",
      "Epoch 13/1000, Train Loss: 0.9855, Val Loss: 1.0050\n",
      "Epoch 14/1000, Train Loss: 0.9759, Val Loss: 0.9971\n",
      "Epoch 15/1000, Train Loss: 0.9685, Val Loss: 0.9912\n",
      "Epoch 16/1000, Train Loss: 0.9654, Val Loss: 0.9860\n",
      "Epoch 17/1000, Train Loss: 0.9575, Val Loss: 0.9815\n",
      "Epoch 18/1000, Train Loss: 0.9537, Val Loss: 0.9773\n",
      "Epoch 19/1000, Train Loss: 0.9536, Val Loss: 0.9750\n",
      "Epoch 20/1000, Train Loss: 0.9430, Val Loss: 0.9707\n",
      "Epoch 21/1000, Train Loss: 0.9453, Val Loss: 0.9682\n",
      "Epoch 22/1000, Train Loss: 0.9407, Val Loss: 0.9649\n",
      "Epoch 23/1000, Train Loss: 0.9359, Val Loss: 0.9615\n",
      "Epoch 24/1000, Train Loss: 0.9362, Val Loss: 0.9590\n",
      "Epoch 25/1000, Train Loss: 0.9357, Val Loss: 0.9560\n",
      "Epoch 26/1000, Train Loss: 0.9322, Val Loss: 0.9530\n",
      "Epoch 27/1000, Train Loss: 0.9281, Val Loss: 0.9495\n",
      "Epoch 28/1000, Train Loss: 0.9244, Val Loss: 0.9464\n",
      "Epoch 29/1000, Train Loss: 0.9223, Val Loss: 0.9427\n",
      "Epoch 30/1000, Train Loss: 0.9182, Val Loss: 0.9381\n",
      "Epoch 31/1000, Train Loss: 0.9148, Val Loss: 0.9338\n",
      "Epoch 32/1000, Train Loss: 0.9109, Val Loss: 0.9286\n",
      "Epoch 33/1000, Train Loss: 0.9032, Val Loss: 0.9235\n",
      "Epoch 34/1000, Train Loss: 0.8982, Val Loss: 0.9154\n",
      "Epoch 35/1000, Train Loss: 0.8981, Val Loss: 0.9088\n",
      "Epoch 36/1000, Train Loss: 0.8909, Val Loss: 0.9023\n",
      "Epoch 37/1000, Train Loss: 0.8850, Val Loss: 0.8969\n",
      "Epoch 38/1000, Train Loss: 0.8776, Val Loss: 0.8917\n",
      "Epoch 39/1000, Train Loss: 0.8724, Val Loss: 0.8800\n",
      "Epoch 40/1000, Train Loss: 0.8636, Val Loss: 0.8742\n",
      "Epoch 41/1000, Train Loss: 0.8560, Val Loss: 0.8641\n",
      "Epoch 42/1000, Train Loss: 0.8514, Val Loss: 0.8557\n",
      "Epoch 43/1000, Train Loss: 0.8420, Val Loss: 0.8488\n",
      "Epoch 44/1000, Train Loss: 0.8317, Val Loss: 0.8407\n",
      "Epoch 45/1000, Train Loss: 0.8289, Val Loss: 0.8350\n",
      "Epoch 46/1000, Train Loss: 0.8254, Val Loss: 0.8261\n",
      "Epoch 47/1000, Train Loss: 0.8148, Val Loss: 0.8192\n",
      "Epoch 48/1000, Train Loss: 0.8080, Val Loss: 0.8134\n",
      "Epoch 49/1000, Train Loss: 0.8006, Val Loss: 0.8054\n",
      "Epoch 50/1000, Train Loss: 0.7921, Val Loss: 0.7981\n",
      "Epoch 51/1000, Train Loss: 0.7857, Val Loss: 0.7903\n",
      "Epoch 52/1000, Train Loss: 0.7826, Val Loss: 0.7824\n",
      "Epoch 53/1000, Train Loss: 0.7744, Val Loss: 0.7754\n",
      "Epoch 54/1000, Train Loss: 0.7684, Val Loss: 0.7680\n",
      "Epoch 55/1000, Train Loss: 0.7580, Val Loss: 0.7615\n",
      "Epoch 56/1000, Train Loss: 0.7512, Val Loss: 0.7526\n",
      "Epoch 57/1000, Train Loss: 0.7439, Val Loss: 0.7446\n",
      "Epoch 58/1000, Train Loss: 0.7369, Val Loss: 0.7374\n",
      "Epoch 59/1000, Train Loss: 0.7327, Val Loss: 0.7312\n",
      "Epoch 60/1000, Train Loss: 0.7244, Val Loss: 0.7228\n",
      "Epoch 61/1000, Train Loss: 0.7179, Val Loss: 0.7154\n",
      "Epoch 62/1000, Train Loss: 0.7073, Val Loss: 0.7081\n",
      "Epoch 63/1000, Train Loss: 0.7005, Val Loss: 0.7004\n",
      "Epoch 64/1000, Train Loss: 0.6911, Val Loss: 0.6908\n",
      "Epoch 65/1000, Train Loss: 0.6894, Val Loss: 0.6927\n",
      "Epoch 66/1000, Train Loss: 0.6800, Val Loss: 0.6773\n",
      "Epoch 67/1000, Train Loss: 0.6744, Val Loss: 0.6746\n",
      "Epoch 68/1000, Train Loss: 0.6703, Val Loss: 0.6674\n",
      "Epoch 69/1000, Train Loss: 0.6622, Val Loss: 0.6586\n",
      "Epoch 70/1000, Train Loss: 0.6531, Val Loss: 0.6519\n",
      "Epoch 71/1000, Train Loss: 0.6472, Val Loss: 0.6465\n",
      "Epoch 72/1000, Train Loss: 0.6481, Val Loss: 0.6450\n",
      "Epoch 73/1000, Train Loss: 0.6354, Val Loss: 0.6419\n",
      "Epoch 74/1000, Train Loss: 0.6330, Val Loss: 0.6321\n",
      "Epoch 75/1000, Train Loss: 0.6285, Val Loss: 0.6280\n",
      "Epoch 76/1000, Train Loss: 0.6211, Val Loss: 0.6251\n",
      "Epoch 77/1000, Train Loss: 0.6195, Val Loss: 0.6222\n",
      "Epoch 78/1000, Train Loss: 0.6168, Val Loss: 0.6179\n",
      "Epoch 79/1000, Train Loss: 0.6105, Val Loss: 0.6161\n",
      "Epoch 80/1000, Train Loss: 0.6107, Val Loss: 0.6148\n",
      "Epoch 81/1000, Train Loss: 0.6029, Val Loss: 0.6125\n",
      "Epoch 82/1000, Train Loss: 0.6002, Val Loss: 0.6046\n",
      "Epoch 83/1000, Train Loss: 0.5984, Val Loss: 0.6051\n",
      "Epoch 84/1000, Train Loss: 0.5911, Val Loss: 0.6063\n",
      "Epoch 85/1000, Train Loss: 0.5896, Val Loss: 0.5972\n",
      "Epoch 86/1000, Train Loss: 0.5878, Val Loss: 0.5969\n",
      "Epoch 87/1000, Train Loss: 0.5859, Val Loss: 0.5942\n",
      "Epoch 88/1000, Train Loss: 0.5824, Val Loss: 0.5950\n",
      "Epoch 89/1000, Train Loss: 0.5834, Val Loss: 0.5885\n",
      "Epoch 90/1000, Train Loss: 0.5766, Val Loss: 0.5872\n",
      "Epoch 91/1000, Train Loss: 0.5787, Val Loss: 0.5917\n",
      "Epoch 92/1000, Train Loss: 0.5745, Val Loss: 0.5837\n",
      "Epoch 93/1000, Train Loss: 0.5741, Val Loss: 0.5820\n",
      "Epoch 94/1000, Train Loss: 0.5716, Val Loss: 0.5835\n",
      "Epoch 95/1000, Train Loss: 0.5684, Val Loss: 0.5814\n",
      "Epoch 96/1000, Train Loss: 0.5664, Val Loss: 0.5836\n",
      "Epoch 97/1000, Train Loss: 0.5675, Val Loss: 0.5806\n",
      "Epoch 98/1000, Train Loss: 0.5685, Val Loss: 0.5759\n",
      "Epoch 99/1000, Train Loss: 0.5642, Val Loss: 0.5806\n",
      "Epoch 100/1000, Train Loss: 0.5636, Val Loss: 0.5742\n",
      "Epoch 101/1000, Train Loss: 0.5594, Val Loss: 0.5742\n",
      "Epoch 102/1000, Train Loss: 0.5618, Val Loss: 0.5728\n",
      "Epoch 103/1000, Train Loss: 0.5585, Val Loss: 0.5729\n",
      "Epoch 104/1000, Train Loss: 0.5592, Val Loss: 0.5744\n",
      "Epoch 105/1000, Train Loss: 0.5554, Val Loss: 0.5680\n",
      "Epoch 106/1000, Train Loss: 0.5554, Val Loss: 0.5708\n",
      "Epoch 107/1000, Train Loss: 0.5584, Val Loss: 0.5676\n",
      "Epoch 108/1000, Train Loss: 0.5549, Val Loss: 0.5670\n",
      "Epoch 109/1000, Train Loss: 0.5515, Val Loss: 0.5657\n",
      "Epoch 110/1000, Train Loss: 0.5497, Val Loss: 0.5681\n",
      "Epoch 111/1000, Train Loss: 0.5504, Val Loss: 0.5667\n",
      "Epoch 112/1000, Train Loss: 0.5495, Val Loss: 0.5670\n",
      "Epoch 113/1000, Train Loss: 0.5468, Val Loss: 0.5722\n",
      "Epoch 114/1000, Train Loss: 0.5468, Val Loss: 0.5705\n",
      "Epoch 115/1000, Train Loss: 0.5460, Val Loss: 0.5679\n",
      "Epoch 116/1000, Train Loss: 0.5473, Val Loss: 0.5705\n",
      "Epoch 117/1000, Train Loss: 0.5462, Val Loss: 0.5596\n",
      "Epoch 118/1000, Train Loss: 0.5459, Val Loss: 0.5627\n",
      "Epoch 119/1000, Train Loss: 0.5455, Val Loss: 0.5684\n",
      "Epoch 120/1000, Train Loss: 0.5447, Val Loss: 0.5669\n",
      "Epoch 121/1000, Train Loss: 0.5441, Val Loss: 0.5563\n",
      "Epoch 122/1000, Train Loss: 0.5433, Val Loss: 0.5562\n",
      "Epoch 123/1000, Train Loss: 0.5403, Val Loss: 0.5578\n",
      "Epoch 124/1000, Train Loss: 0.5394, Val Loss: 0.5570\n",
      "Epoch 125/1000, Train Loss: 0.5404, Val Loss: 0.5569\n",
      "Epoch 126/1000, Train Loss: 0.5410, Val Loss: 0.5568\n",
      "Epoch 127/1000, Train Loss: 0.5431, Val Loss: 0.5568\n",
      "Epoch 128/1000, Train Loss: 0.5394, Val Loss: 0.5542\n",
      "Epoch 129/1000, Train Loss: 0.5358, Val Loss: 0.5536\n",
      "Epoch 130/1000, Train Loss: 0.5367, Val Loss: 0.5577\n",
      "Epoch 131/1000, Train Loss: 0.5375, Val Loss: 0.5540\n",
      "Epoch 132/1000, Train Loss: 0.5365, Val Loss: 0.5532\n",
      "Epoch 133/1000, Train Loss: 0.5359, Val Loss: 0.5527\n",
      "Epoch 134/1000, Train Loss: 0.5336, Val Loss: 0.5545\n",
      "Epoch 135/1000, Train Loss: 0.5368, Val Loss: 0.5549\n",
      "Epoch 136/1000, Train Loss: 0.5321, Val Loss: 0.5499\n",
      "Epoch 137/1000, Train Loss: 0.5326, Val Loss: 0.5537\n",
      "Epoch 138/1000, Train Loss: 0.5325, Val Loss: 0.5546\n",
      "Epoch 139/1000, Train Loss: 0.5336, Val Loss: 0.5603\n",
      "Epoch 140/1000, Train Loss: 0.5322, Val Loss: 0.5494\n",
      "Epoch 141/1000, Train Loss: 0.5307, Val Loss: 0.5513\n",
      "Epoch 142/1000, Train Loss: 0.5316, Val Loss: 0.5520\n",
      "Epoch 143/1000, Train Loss: 0.5309, Val Loss: 0.5502\n",
      "Epoch 144/1000, Train Loss: 0.5301, Val Loss: 0.5530\n",
      "Epoch 145/1000, Train Loss: 0.5294, Val Loss: 0.5470\n",
      "Epoch 146/1000, Train Loss: 0.5313, Val Loss: 0.5465\n",
      "Epoch 147/1000, Train Loss: 0.5269, Val Loss: 0.5461\n",
      "Epoch 148/1000, Train Loss: 0.5280, Val Loss: 0.5460\n",
      "Epoch 149/1000, Train Loss: 0.5289, Val Loss: 0.5445\n",
      "Epoch 150/1000, Train Loss: 0.5300, Val Loss: 0.5520\n",
      "Epoch 151/1000, Train Loss: 0.5291, Val Loss: 0.5443\n",
      "Epoch 152/1000, Train Loss: 0.5286, Val Loss: 0.5416\n",
      "Epoch 153/1000, Train Loss: 0.5320, Val Loss: 0.5448\n",
      "Epoch 154/1000, Train Loss: 0.5273, Val Loss: 0.5427\n",
      "Epoch 155/1000, Train Loss: 0.5286, Val Loss: 0.5463\n",
      "Epoch 156/1000, Train Loss: 0.5307, Val Loss: 0.5385\n",
      "Epoch 157/1000, Train Loss: 0.5259, Val Loss: 0.5356\n",
      "Epoch 158/1000, Train Loss: 0.5257, Val Loss: 0.5352\n",
      "Epoch 159/1000, Train Loss: 0.5254, Val Loss: 0.5357\n",
      "Epoch 160/1000, Train Loss: 0.5226, Val Loss: 0.5395\n",
      "Epoch 161/1000, Train Loss: 0.5243, Val Loss: 0.5454\n",
      "Epoch 162/1000, Train Loss: 0.5233, Val Loss: 0.5416\n",
      "Epoch 163/1000, Train Loss: 0.5222, Val Loss: 0.5361\n",
      "Epoch 164/1000, Train Loss: 0.5216, Val Loss: 0.5363\n",
      "Epoch 165/1000, Train Loss: 0.5188, Val Loss: 0.5421\n",
      "Epoch 166/1000, Train Loss: 0.5216, Val Loss: 0.5413\n",
      "Epoch 167/1000, Train Loss: 0.5203, Val Loss: 0.5327\n",
      "Epoch 168/1000, Train Loss: 0.5180, Val Loss: 0.5351\n",
      "Epoch 169/1000, Train Loss: 0.5208, Val Loss: 0.5320\n",
      "Epoch 170/1000, Train Loss: 0.5193, Val Loss: 0.5319\n",
      "Epoch 171/1000, Train Loss: 0.5173, Val Loss: 0.5479\n",
      "Epoch 172/1000, Train Loss: 0.5193, Val Loss: 0.5377\n",
      "Epoch 173/1000, Train Loss: 0.5230, Val Loss: 0.5329\n",
      "Epoch 174/1000, Train Loss: 0.5153, Val Loss: 0.5316\n",
      "Epoch 175/1000, Train Loss: 0.5168, Val Loss: 0.5314\n",
      "Epoch 176/1000, Train Loss: 0.5173, Val Loss: 0.5386\n",
      "Epoch 177/1000, Train Loss: 0.5144, Val Loss: 0.5389\n",
      "Epoch 178/1000, Train Loss: 0.5172, Val Loss: 0.5263\n",
      "Epoch 179/1000, Train Loss: 0.5134, Val Loss: 0.5411\n",
      "Epoch 180/1000, Train Loss: 0.5153, Val Loss: 0.5258\n",
      "Epoch 181/1000, Train Loss: 0.5165, Val Loss: 0.5312\n",
      "Epoch 182/1000, Train Loss: 0.5127, Val Loss: 0.5294\n",
      "Epoch 183/1000, Train Loss: 0.5118, Val Loss: 0.5197\n",
      "Epoch 184/1000, Train Loss: 0.5154, Val Loss: 0.5253\n",
      "Epoch 185/1000, Train Loss: 0.5112, Val Loss: 0.5328\n",
      "Epoch 186/1000, Train Loss: 0.5120, Val Loss: 0.5206\n",
      "Epoch 187/1000, Train Loss: 0.5136, Val Loss: 0.5235\n",
      "Epoch 188/1000, Train Loss: 0.5102, Val Loss: 0.5257\n",
      "Epoch 189/1000, Train Loss: 0.5128, Val Loss: 0.5272\n",
      "Epoch 190/1000, Train Loss: 0.5102, Val Loss: 0.5249\n",
      "Epoch 191/1000, Train Loss: 0.5092, Val Loss: 0.5263\n",
      "Epoch 192/1000, Train Loss: 0.5109, Val Loss: 0.5196\n",
      "Epoch 193/1000, Train Loss: 0.5083, Val Loss: 0.5179\n",
      "Epoch 194/1000, Train Loss: 0.5119, Val Loss: 0.5223\n",
      "Epoch 195/1000, Train Loss: 0.5117, Val Loss: 0.5243\n",
      "Epoch 196/1000, Train Loss: 0.5072, Val Loss: 0.5193\n",
      "Epoch 197/1000, Train Loss: 0.5075, Val Loss: 0.5192\n",
      "Epoch 198/1000, Train Loss: 0.5092, Val Loss: 0.5160\n",
      "Epoch 199/1000, Train Loss: 0.5082, Val Loss: 0.5186\n",
      "Epoch 200/1000, Train Loss: 0.5053, Val Loss: 0.5294\n",
      "Epoch 201/1000, Train Loss: 0.5073, Val Loss: 0.5205\n",
      "Epoch 202/1000, Train Loss: 0.5107, Val Loss: 0.5192\n",
      "Epoch 203/1000, Train Loss: 0.5067, Val Loss: 0.5178\n",
      "Epoch 204/1000, Train Loss: 0.5085, Val Loss: 0.5254\n",
      "Epoch 205/1000, Train Loss: 0.5052, Val Loss: 0.5232\n",
      "Epoch 206/1000, Train Loss: 0.5092, Val Loss: 0.5175\n",
      "Epoch 207/1000, Train Loss: 0.5061, Val Loss: 0.5153\n",
      "Epoch 208/1000, Train Loss: 0.5042, Val Loss: 0.5146\n",
      "Epoch 209/1000, Train Loss: 0.5030, Val Loss: 0.5163\n",
      "Epoch 210/1000, Train Loss: 0.5097, Val Loss: 0.5175\n",
      "Epoch 211/1000, Train Loss: 0.5085, Val Loss: 0.5163\n",
      "Epoch 212/1000, Train Loss: 0.5057, Val Loss: 0.5196\n",
      "Epoch 213/1000, Train Loss: 0.5055, Val Loss: 0.5266\n",
      "Epoch 214/1000, Train Loss: 0.5062, Val Loss: 0.5185\n",
      "Epoch 215/1000, Train Loss: 0.5068, Val Loss: 0.5135\n",
      "Epoch 216/1000, Train Loss: 0.5075, Val Loss: 0.5169\n",
      "Epoch 217/1000, Train Loss: 0.5072, Val Loss: 0.5168\n",
      "Epoch 218/1000, Train Loss: 0.5049, Val Loss: 0.5114\n",
      "Epoch 219/1000, Train Loss: 0.5041, Val Loss: 0.5285\n",
      "Epoch 220/1000, Train Loss: 0.5052, Val Loss: 0.5137\n",
      "Epoch 221/1000, Train Loss: 0.5019, Val Loss: 0.5200\n",
      "Epoch 222/1000, Train Loss: 0.5043, Val Loss: 0.5196\n",
      "Epoch 223/1000, Train Loss: 0.5026, Val Loss: 0.5175\n",
      "Epoch 224/1000, Train Loss: 0.5031, Val Loss: 0.5162\n",
      "Epoch 225/1000, Train Loss: 0.5023, Val Loss: 0.5210\n",
      "Epoch 226/1000, Train Loss: 0.5028, Val Loss: 0.5154\n",
      "Epoch 227/1000, Train Loss: 0.5006, Val Loss: 0.5159\n",
      "Epoch 228/1000, Train Loss: 0.5015, Val Loss: 0.5143\n",
      "Epoch 229/1000, Train Loss: 0.5016, Val Loss: 0.5121\n",
      "Epoch 230/1000, Train Loss: 0.5041, Val Loss: 0.5141\n",
      "Epoch 231/1000, Train Loss: 0.5042, Val Loss: 0.5141\n",
      "Epoch 232/1000, Train Loss: 0.5029, Val Loss: 0.5137\n",
      "Epoch 233/1000, Train Loss: 0.5011, Val Loss: 0.5117\n",
      "Epoch 234/1000, Train Loss: 0.5003, Val Loss: 0.5150\n",
      "Epoch 235/1000, Train Loss: 0.5012, Val Loss: 0.5156\n",
      "Epoch 236/1000, Train Loss: 0.5045, Val Loss: 0.5175\n",
      "Epoch 237/1000, Train Loss: 0.5007, Val Loss: 0.5160\n",
      "Epoch 238/1000, Train Loss: 0.5035, Val Loss: 0.5150\n",
      "Epoch 239/1000, Train Loss: 0.5034, Val Loss: 0.5183\n",
      "Epoch 240/1000, Train Loss: 0.5000, Val Loss: 0.5109\n",
      "Epoch 241/1000, Train Loss: 0.5083, Val Loss: 0.5100\n",
      "Epoch 242/1000, Train Loss: 0.4984, Val Loss: 0.5149\n",
      "Epoch 243/1000, Train Loss: 0.4985, Val Loss: 0.5130\n",
      "Epoch 244/1000, Train Loss: 0.4995, Val Loss: 0.5159\n",
      "Epoch 245/1000, Train Loss: 0.5013, Val Loss: 0.5154\n",
      "Epoch 246/1000, Train Loss: 0.5020, Val Loss: 0.5122\n",
      "Epoch 247/1000, Train Loss: 0.4981, Val Loss: 0.5091\n",
      "Epoch 248/1000, Train Loss: 0.5002, Val Loss: 0.5179\n",
      "Epoch 249/1000, Train Loss: 0.4996, Val Loss: 0.5130\n",
      "Epoch 250/1000, Train Loss: 0.4981, Val Loss: 0.5134\n",
      "Epoch 251/1000, Train Loss: 0.5024, Val Loss: 0.5143\n",
      "Epoch 252/1000, Train Loss: 0.4996, Val Loss: 0.5095\n",
      "Epoch 253/1000, Train Loss: 0.5010, Val Loss: 0.5196\n",
      "Epoch 254/1000, Train Loss: 0.5018, Val Loss: 0.5131\n",
      "Epoch 255/1000, Train Loss: 0.5040, Val Loss: 0.5123\n",
      "Epoch 256/1000, Train Loss: 0.5004, Val Loss: 0.5135\n",
      "Epoch 257/1000, Train Loss: 0.4989, Val Loss: 0.5171\n",
      "Epoch 258/1000, Train Loss: 0.5077, Val Loss: 0.5138\n",
      "Epoch 259/1000, Train Loss: 0.5006, Val Loss: 0.5105\n",
      "Epoch 260/1000, Train Loss: 0.4989, Val Loss: 0.5089\n",
      "Epoch 261/1000, Train Loss: 0.4984, Val Loss: 0.5111\n",
      "Epoch 262/1000, Train Loss: 0.4992, Val Loss: 0.5103\n",
      "Epoch 263/1000, Train Loss: 0.4993, Val Loss: 0.5088\n",
      "Epoch 264/1000, Train Loss: 0.4977, Val Loss: 0.5095\n",
      "Epoch 265/1000, Train Loss: 0.4988, Val Loss: 0.5085\n",
      "Epoch 266/1000, Train Loss: 0.4962, Val Loss: 0.5078\n",
      "Epoch 267/1000, Train Loss: 0.4964, Val Loss: 0.5168\n",
      "Epoch 268/1000, Train Loss: 0.5007, Val Loss: 0.5106\n",
      "Epoch 269/1000, Train Loss: 0.4953, Val Loss: 0.5088\n",
      "Epoch 270/1000, Train Loss: 0.4972, Val Loss: 0.5115\n",
      "Epoch 271/1000, Train Loss: 0.4977, Val Loss: 0.5199\n",
      "Epoch 272/1000, Train Loss: 0.4977, Val Loss: 0.5076\n",
      "Epoch 273/1000, Train Loss: 0.4993, Val Loss: 0.5098\n",
      "Epoch 274/1000, Train Loss: 0.4978, Val Loss: 0.5112\n",
      "Epoch 275/1000, Train Loss: 0.4962, Val Loss: 0.5082\n",
      "Epoch 276/1000, Train Loss: 0.4970, Val Loss: 0.5063\n",
      "Epoch 277/1000, Train Loss: 0.4969, Val Loss: 0.5090\n",
      "Epoch 278/1000, Train Loss: 0.4946, Val Loss: 0.5169\n",
      "Epoch 279/1000, Train Loss: 0.4977, Val Loss: 0.5079\n",
      "Epoch 280/1000, Train Loss: 0.4973, Val Loss: 0.5045\n",
      "Epoch 281/1000, Train Loss: 0.4952, Val Loss: 0.5156\n",
      "Epoch 282/1000, Train Loss: 0.4961, Val Loss: 0.5120\n",
      "Epoch 283/1000, Train Loss: 0.4977, Val Loss: 0.5056\n",
      "Epoch 284/1000, Train Loss: 0.4965, Val Loss: 0.5040\n",
      "Epoch 285/1000, Train Loss: 0.4970, Val Loss: 0.5071\n",
      "Epoch 286/1000, Train Loss: 0.4963, Val Loss: 0.5053\n",
      "Epoch 287/1000, Train Loss: 0.4946, Val Loss: 0.5149\n",
      "Epoch 288/1000, Train Loss: 0.4997, Val Loss: 0.5136\n",
      "Epoch 289/1000, Train Loss: 0.5021, Val Loss: 0.5096\n",
      "Epoch 290/1000, Train Loss: 0.4959, Val Loss: 0.5047\n",
      "Epoch 291/1000, Train Loss: 0.4943, Val Loss: 0.5039\n",
      "Epoch 292/1000, Train Loss: 0.4941, Val Loss: 0.5101\n",
      "Epoch 293/1000, Train Loss: 0.4937, Val Loss: 0.5037\n",
      "Epoch 294/1000, Train Loss: 0.4969, Val Loss: 0.5057\n",
      "Epoch 295/1000, Train Loss: 0.4923, Val Loss: 0.5082\n",
      "Epoch 296/1000, Train Loss: 0.4958, Val Loss: 0.5074\n",
      "Epoch 297/1000, Train Loss: 0.4948, Val Loss: 0.5083\n",
      "Epoch 298/1000, Train Loss: 0.4957, Val Loss: 0.5069\n",
      "Epoch 299/1000, Train Loss: 0.4951, Val Loss: 0.5125\n",
      "Epoch 300/1000, Train Loss: 0.4934, Val Loss: 0.5091\n",
      "Epoch 301/1000, Train Loss: 0.4939, Val Loss: 0.5105\n",
      "Epoch 302/1000, Train Loss: 0.4935, Val Loss: 0.5083\n",
      "Epoch 303/1000, Train Loss: 0.4948, Val Loss: 0.5093\n",
      "Epoch 304/1000, Train Loss: 0.4952, Val Loss: 0.5085\n",
      "Epoch 305/1000, Train Loss: 0.4919, Val Loss: 0.5045\n",
      "Epoch 306/1000, Train Loss: 0.4940, Val Loss: 0.5078\n",
      "Epoch 307/1000, Train Loss: 0.4947, Val Loss: 0.5107\n",
      "Epoch 308/1000, Train Loss: 0.4939, Val Loss: 0.5059\n",
      "Epoch 309/1000, Train Loss: 0.4944, Val Loss: 0.5108\n",
      "Epoch 310/1000, Train Loss: 0.4945, Val Loss: 0.5027\n",
      "Epoch 311/1000, Train Loss: 0.4943, Val Loss: 0.5055\n",
      "Epoch 312/1000, Train Loss: 0.4956, Val Loss: 0.5032\n",
      "Epoch 313/1000, Train Loss: 0.4920, Val Loss: 0.5058\n",
      "Epoch 314/1000, Train Loss: 0.4909, Val Loss: 0.5081\n",
      "Epoch 315/1000, Train Loss: 0.4934, Val Loss: 0.5099\n",
      "Epoch 316/1000, Train Loss: 0.4928, Val Loss: 0.5029\n",
      "Epoch 317/1000, Train Loss: 0.4927, Val Loss: 0.5037\n",
      "Epoch 318/1000, Train Loss: 0.4911, Val Loss: 0.5065\n",
      "Epoch 319/1000, Train Loss: 0.4893, Val Loss: 0.5011\n",
      "Epoch 320/1000, Train Loss: 0.4926, Val Loss: 0.5031\n",
      "Epoch 321/1000, Train Loss: 0.4901, Val Loss: 0.5048\n",
      "Epoch 322/1000, Train Loss: 0.4933, Val Loss: 0.5023\n",
      "Epoch 323/1000, Train Loss: 0.4886, Val Loss: 0.5117\n",
      "Epoch 324/1000, Train Loss: 0.4934, Val Loss: 0.5041\n",
      "Epoch 325/1000, Train Loss: 0.4946, Val Loss: 0.5116\n",
      "Epoch 326/1000, Train Loss: 0.4958, Val Loss: 0.5078\n",
      "Epoch 327/1000, Train Loss: 0.4905, Val Loss: 0.5035\n",
      "Epoch 328/1000, Train Loss: 0.4904, Val Loss: 0.5049\n",
      "Epoch 329/1000, Train Loss: 0.4918, Val Loss: 0.5049\n",
      "Epoch 330/1000, Train Loss: 0.4914, Val Loss: 0.5044\n",
      "Epoch 331/1000, Train Loss: 0.4914, Val Loss: 0.5097\n",
      "Epoch 332/1000, Train Loss: 0.4902, Val Loss: 0.5141\n",
      "Epoch 333/1000, Train Loss: 0.4919, Val Loss: 0.5051\n",
      "Epoch 334/1000, Train Loss: 0.4901, Val Loss: 0.5016\n",
      "Epoch 335/1000, Train Loss: 0.4869, Val Loss: 0.5076\n",
      "Epoch 336/1000, Train Loss: 0.4885, Val Loss: 0.5117\n",
      "Epoch 337/1000, Train Loss: 0.4942, Val Loss: 0.5021\n",
      "Epoch 338/1000, Train Loss: 0.4904, Val Loss: 0.5080\n",
      "Epoch 339/1000, Train Loss: 0.4920, Val Loss: 0.5005\n",
      "Epoch 340/1000, Train Loss: 0.4900, Val Loss: 0.5051\n",
      "Epoch 341/1000, Train Loss: 0.4871, Val Loss: 0.5012\n",
      "Epoch 342/1000, Train Loss: 0.4908, Val Loss: 0.5024\n",
      "Epoch 343/1000, Train Loss: 0.4889, Val Loss: 0.5091\n",
      "Epoch 344/1000, Train Loss: 0.4895, Val Loss: 0.5066\n",
      "Epoch 345/1000, Train Loss: 0.4869, Val Loss: 0.5041\n",
      "Epoch 346/1000, Train Loss: 0.4931, Val Loss: 0.5046\n",
      "Epoch 347/1000, Train Loss: 0.4882, Val Loss: 0.5040\n",
      "Epoch 348/1000, Train Loss: 0.4872, Val Loss: 0.5046\n",
      "Epoch 349/1000, Train Loss: 0.4857, Val Loss: 0.5017\n",
      "Epoch 350/1000, Train Loss: 0.4913, Val Loss: 0.5043\n",
      "Epoch 351/1000, Train Loss: 0.4869, Val Loss: 0.5110\n",
      "Epoch 352/1000, Train Loss: 0.4866, Val Loss: 0.5013\n",
      "Epoch 353/1000, Train Loss: 0.4885, Val Loss: 0.4997\n",
      "Epoch 354/1000, Train Loss: 0.4877, Val Loss: 0.5008\n",
      "Epoch 355/1000, Train Loss: 0.4860, Val Loss: 0.5011\n",
      "Epoch 356/1000, Train Loss: 0.4875, Val Loss: 0.5080\n",
      "Epoch 357/1000, Train Loss: 0.4859, Val Loss: 0.5005\n",
      "Epoch 358/1000, Train Loss: 0.4867, Val Loss: 0.5030\n",
      "Epoch 359/1000, Train Loss: 0.4851, Val Loss: 0.5043\n",
      "Epoch 360/1000, Train Loss: 0.4880, Val Loss: 0.5026\n",
      "Epoch 361/1000, Train Loss: 0.4845, Val Loss: 0.5042\n",
      "Epoch 362/1000, Train Loss: 0.4882, Val Loss: 0.5060\n",
      "Epoch 363/1000, Train Loss: 0.4878, Val Loss: 0.5023\n",
      "Epoch 364/1000, Train Loss: 0.4867, Val Loss: 0.5012\n",
      "Epoch 365/1000, Train Loss: 0.4850, Val Loss: 0.4981\n",
      "Epoch 366/1000, Train Loss: 0.4877, Val Loss: 0.5098\n",
      "Epoch 367/1000, Train Loss: 0.4890, Val Loss: 0.5007\n",
      "Epoch 368/1000, Train Loss: 0.4867, Val Loss: 0.5011\n",
      "Epoch 369/1000, Train Loss: 0.4863, Val Loss: 0.4962\n",
      "Epoch 370/1000, Train Loss: 0.4869, Val Loss: 0.5009\n",
      "Epoch 371/1000, Train Loss: 0.4837, Val Loss: 0.5014\n",
      "Epoch 372/1000, Train Loss: 0.4852, Val Loss: 0.4970\n",
      "Epoch 373/1000, Train Loss: 0.4847, Val Loss: 0.4988\n",
      "Epoch 374/1000, Train Loss: 0.4838, Val Loss: 0.5050\n",
      "Epoch 375/1000, Train Loss: 0.4874, Val Loss: 0.5016\n",
      "Epoch 376/1000, Train Loss: 0.4836, Val Loss: 0.4992\n",
      "Epoch 377/1000, Train Loss: 0.4842, Val Loss: 0.4999\n",
      "Epoch 378/1000, Train Loss: 0.4841, Val Loss: 0.5044\n",
      "Epoch 379/1000, Train Loss: 0.4835, Val Loss: 0.5004\n",
      "Epoch 380/1000, Train Loss: 0.4804, Val Loss: 0.4956\n",
      "Epoch 381/1000, Train Loss: 0.4834, Val Loss: 0.5052\n",
      "Epoch 382/1000, Train Loss: 0.4831, Val Loss: 0.5008\n",
      "Epoch 383/1000, Train Loss: 0.4852, Val Loss: 0.4963\n",
      "Epoch 384/1000, Train Loss: 0.4830, Val Loss: 0.4971\n",
      "Epoch 385/1000, Train Loss: 0.4822, Val Loss: 0.4959\n",
      "Epoch 386/1000, Train Loss: 0.4825, Val Loss: 0.4968\n",
      "Epoch 387/1000, Train Loss: 0.4793, Val Loss: 0.5021\n",
      "Epoch 388/1000, Train Loss: 0.4823, Val Loss: 0.4984\n",
      "Epoch 389/1000, Train Loss: 0.4812, Val Loss: 0.4944\n",
      "Epoch 390/1000, Train Loss: 0.4800, Val Loss: 0.4963\n",
      "Epoch 391/1000, Train Loss: 0.4780, Val Loss: 0.4957\n",
      "Epoch 392/1000, Train Loss: 0.4803, Val Loss: 0.4992\n",
      "Epoch 393/1000, Train Loss: 0.4791, Val Loss: 0.4958\n",
      "Epoch 394/1000, Train Loss: 0.4823, Val Loss: 0.4994\n",
      "Epoch 395/1000, Train Loss: 0.4822, Val Loss: 0.4947\n",
      "Epoch 396/1000, Train Loss: 0.4811, Val Loss: 0.4968\n",
      "Epoch 397/1000, Train Loss: 0.4824, Val Loss: 0.5000\n",
      "Epoch 398/1000, Train Loss: 0.4804, Val Loss: 0.5016\n",
      "Epoch 399/1000, Train Loss: 0.4791, Val Loss: 0.4971\n",
      "Epoch 400/1000, Train Loss: 0.4784, Val Loss: 0.4983\n",
      "Epoch 401/1000, Train Loss: 0.4818, Val Loss: 0.4953\n",
      "Epoch 402/1000, Train Loss: 0.4782, Val Loss: 0.5077\n",
      "Epoch 403/1000, Train Loss: 0.4831, Val Loss: 0.4956\n",
      "Epoch 404/1000, Train Loss: 0.4781, Val Loss: 0.4961\n",
      "Epoch 405/1000, Train Loss: 0.4793, Val Loss: 0.4956\n",
      "Epoch 406/1000, Train Loss: 0.4775, Val Loss: 0.4962\n",
      "Epoch 407/1000, Train Loss: 0.4778, Val Loss: 0.4971\n",
      "Epoch 408/1000, Train Loss: 0.4749, Val Loss: 0.4918\n",
      "Epoch 409/1000, Train Loss: 0.4746, Val Loss: 0.4966\n",
      "Epoch 410/1000, Train Loss: 0.4751, Val Loss: 0.4928\n",
      "Epoch 411/1000, Train Loss: 0.4759, Val Loss: 0.4923\n",
      "Epoch 412/1000, Train Loss: 0.4769, Val Loss: 0.4901\n",
      "Epoch 413/1000, Train Loss: 0.4761, Val Loss: 0.4921\n",
      "Epoch 414/1000, Train Loss: 0.4749, Val Loss: 0.4944\n",
      "Epoch 415/1000, Train Loss: 0.4779, Val Loss: 0.4947\n",
      "Epoch 416/1000, Train Loss: 0.4751, Val Loss: 0.4917\n",
      "Epoch 417/1000, Train Loss: 0.4724, Val Loss: 0.5020\n",
      "Epoch 418/1000, Train Loss: 0.4753, Val Loss: 0.4911\n",
      "Epoch 419/1000, Train Loss: 0.4767, Val Loss: 0.4919\n",
      "Epoch 420/1000, Train Loss: 0.4749, Val Loss: 0.4931\n",
      "Epoch 421/1000, Train Loss: 0.4733, Val Loss: 0.4985\n",
      "Epoch 422/1000, Train Loss: 0.4750, Val Loss: 0.4902\n",
      "Epoch 423/1000, Train Loss: 0.4704, Val Loss: 0.4914\n",
      "Epoch 424/1000, Train Loss: 0.4727, Val Loss: 0.4923\n",
      "Epoch 425/1000, Train Loss: 0.4721, Val Loss: 0.4908\n",
      "Epoch 426/1000, Train Loss: 0.4727, Val Loss: 0.4913\n",
      "Epoch 427/1000, Train Loss: 0.4742, Val Loss: 0.4901\n",
      "Epoch 428/1000, Train Loss: 0.4720, Val Loss: 0.4993\n",
      "Epoch 429/1000, Train Loss: 0.4739, Val Loss: 0.4897\n",
      "Epoch 430/1000, Train Loss: 0.4729, Val Loss: 0.4916\n",
      "Epoch 431/1000, Train Loss: 0.4715, Val Loss: 0.4908\n",
      "Epoch 432/1000, Train Loss: 0.4715, Val Loss: 0.4928\n",
      "Epoch 433/1000, Train Loss: 0.4742, Val Loss: 0.4913\n",
      "Epoch 434/1000, Train Loss: 0.4702, Val Loss: 0.4895\n",
      "Epoch 435/1000, Train Loss: 0.4716, Val Loss: 0.4899\n",
      "Epoch 436/1000, Train Loss: 0.4680, Val Loss: 0.4943\n",
      "Epoch 437/1000, Train Loss: 0.4667, Val Loss: 0.4910\n",
      "Epoch 438/1000, Train Loss: 0.4674, Val Loss: 0.4933\n",
      "Epoch 439/1000, Train Loss: 0.4675, Val Loss: 0.4887\n",
      "Epoch 440/1000, Train Loss: 0.4705, Val Loss: 0.5025\n",
      "Epoch 441/1000, Train Loss: 0.4705, Val Loss: 0.4932\n",
      "Epoch 442/1000, Train Loss: 0.4684, Val Loss: 0.4848\n",
      "Epoch 443/1000, Train Loss: 0.4667, Val Loss: 0.4910\n",
      "Epoch 444/1000, Train Loss: 0.4674, Val Loss: 0.4897\n",
      "Epoch 445/1000, Train Loss: 0.4697, Val Loss: 0.4891\n",
      "Epoch 446/1000, Train Loss: 0.4692, Val Loss: 0.4848\n",
      "Epoch 447/1000, Train Loss: 0.4679, Val Loss: 0.4883\n",
      "Epoch 448/1000, Train Loss: 0.4675, Val Loss: 0.4834\n",
      "Epoch 449/1000, Train Loss: 0.4639, Val Loss: 0.4879\n",
      "Epoch 450/1000, Train Loss: 0.4650, Val Loss: 0.4835\n",
      "Epoch 451/1000, Train Loss: 0.4660, Val Loss: 0.4840\n",
      "Epoch 452/1000, Train Loss: 0.4621, Val Loss: 0.4873\n",
      "Epoch 453/1000, Train Loss: 0.4631, Val Loss: 0.4827\n",
      "Epoch 454/1000, Train Loss: 0.4670, Val Loss: 0.4849\n",
      "Epoch 455/1000, Train Loss: 0.4642, Val Loss: 0.4839\n",
      "Epoch 456/1000, Train Loss: 0.4646, Val Loss: 0.4870\n",
      "Epoch 457/1000, Train Loss: 0.4654, Val Loss: 0.4835\n",
      "Epoch 458/1000, Train Loss: 0.4611, Val Loss: 0.4819\n",
      "Epoch 459/1000, Train Loss: 0.4612, Val Loss: 0.4871\n"
     ]
    }
   ],
   "source": [
    "# 필요한 라이브러리 임포트\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 사용할 특성 선택\n",
    "features = ['season', 'holiday', 'workingday', 'weather', 'temp', \n",
    "            'atemp', 'humidity', 'windspeed', 'year', 'month', \n",
    "            'day', 'hour', 'dayofweek']\n",
    "\n",
    "# 데이터 전처리 및 정규화 함수 정의\n",
    "def preprocess_and_scale(train_data, test_data, features):\n",
    "    # datetime에서 유용한 특성 추출\n",
    "    for dataset in [train_data, test_data]:\n",
    "        dataset['datetime'] = pd.to_datetime(dataset['datetime'])\n",
    "        dataset['year'] = dataset['datetime'].dt.year\n",
    "        dataset['month'] = dataset['datetime'].dt.month\n",
    "        dataset['day'] = dataset['datetime'].dt.day\n",
    "        dataset['hour'] = dataset['datetime'].dt.hour\n",
    "        dataset['dayofweek'] = dataset['datetime'].dt.dayofweek\n",
    "    \n",
    "    # StandardScaler를 사용해 데이터 정규화\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(train_data[features])\n",
    "    X_test = scaler.transform(test_data[features])\n",
    "\n",
    "    return X_train, X_test, scaler\n",
    "\n",
    "# RMSLE 손실 함수 정의 (0 이하 값 방지를 위한 epsilon 추가)\n",
    "class RMSLELoss(nn.Module):\n",
    "    def __init__(self, epsilon=1e-6):\n",
    "        super(RMSLELoss, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        # y_pred가 0 이하가 되지 않도록 epsilon 추가\n",
    "        return torch.sqrt(torch.mean((torch.log1p(y_pred + self.epsilon) - torch.log1p(y_true + self.epsilon)) ** 2))\n",
    "\n",
    "# 데이터 불러오기\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "y = train['count'].values\n",
    "\n",
    "# 데이터 전처리 및 정규화\n",
    "X_train, X_test, scaler = preprocess_and_scale(train, test, features)\n",
    "\n",
    "# 학습/검증 데이터 분리\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 데이터셋을 텐서로 변환\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "# DataLoader 설정\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# 모델 정의\n",
    "class BikeDemandModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BikeDemandModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(len(features), 32)\n",
    "        self.fc2 = nn.Linear(32, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return torch.relu(x)  # 출력에 ReLU 적용\n",
    "\n",
    "# 모델 초기화\n",
    "model = BikeDemandModel()\n",
    "criterion = RMSLELoss(epsilon=0)  # 손실 함수에 epsilon 설정\n",
    "#criterion = RMSLELoss(epsilon=1e-6)  # 손실 함수에 epsilon 설정\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 함수 정의\n",
    "def train_model(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        \n",
    "        # 손실 계산 및 역전파\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / len(train_loader)\n",
    "\n",
    "# 검증 함수 정의\n",
    "def evaluate_model(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            outputs = model(X_batch)\n",
    "            # 예측값 후처리: 정수로 반올림하고 음수는 0으로 변환\n",
    "            outputs = torch.round(outputs).clamp(min=0)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            val_loss += loss.item()\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "# 모델 저장 경로\n",
    "best_model_path = \"best_model.pth\"\n",
    "\n",
    "# 모델 학습 및 검증\n",
    "n_epochs = 1000\n",
    "best_val_loss = float('inf')  # 초기값을 무한대로 설정\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_model(model, train_loader, criterion, optimizer)\n",
    "    val_loss = evaluate_model(model, val_loader, criterion)\n",
    "    print(f'Epoch {epoch+1}/{n_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)  # 최적 모델 저장\n",
    "\n",
    "# 최적 모델 불러오기\n",
    "model.load_state_dict(torch.load(best_model_path))\n",
    "print(\"Best model loaded for evaluation.\")\n",
    "\n",
    "# 검증 데이터로 성능 평가\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    val_pred = model(X_val_tensor).numpy()\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, val_pred))\n",
    "    mae = mean_absolute_error(y_val, val_pred)\n",
    "    print('검증 데이터 RMSE:', rmse)\n",
    "    print('검증 데이터 MAE:', mae)\n",
    "\n",
    "# 테스트 데이터 예측\n",
    "with torch.no_grad():\n",
    "    test_pred = model(X_test_tensor)\n",
    "    test_pred = torch.round(test_pred).clamp(min=0).numpy()\n",
    "\n",
    "# 제출 파일 생성\n",
    "submission = pd.DataFrame({\n",
    "    'datetime': test['datetime'],\n",
    "    'count': test_pred.flatten()\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print('제출 파일이 생성되었습니다.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
